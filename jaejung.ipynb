{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opt():\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--path', default='./data/photoshapes/shape09096_rank00',type=str)\n",
    "    parser.add_argument('-O', action='store_true', help=\"equals --fp16 --cuda_ray --preload\")\n",
    "    parser.add_argument('--test', action='store_true', help=\"test mode\")\n",
    "    parser.add_argument('--workspace', type=str, default='workspace')\n",
    "    parser.add_argument('--seed', type=int, default=0)\n",
    "    ### training options\n",
    "    parser.add_argument('--iters', type=int, default=15000, help=\"training iters\")\n",
    "    parser.add_argument('--lr', type=float, default=1e-2, help=\"initial learning rate\")\n",
    "    parser.add_argument('--ckpt', type=str, default='best')\n",
    "    parser.add_argument('--num_rays', type=int, default=4096, help=\"num rays sampled per image for each training step\")\n",
    "    parser.add_argument('--cuda_ray', action='store_true', help=\"use CUDA raymarching instead of pytorch\")\n",
    "    parser.add_argument('--max_steps', type=int, default=1024, help=\"max num steps sampled per ray (only valid when using --cuda_ray)\")\n",
    "    parser.add_argument('--num_steps', type=int, default=512, help=\"num steps sampled per ray (only valid when NOT using --cuda_ray)\")\n",
    "    parser.add_argument('--upsample_steps', type=int, default=0, help=\"num steps up-sampled per ray (only valid when NOT using --cuda_ray)\")\n",
    "    parser.add_argument('--update_extra_interval', type=int, default=16, help=\"iter interval to update extra status (only valid when using --cuda_ray)\")\n",
    "    parser.add_argument('--max_ray_batch', type=int, default=4096, help=\"batch size of rays at inference to avoid OOM (only valid when NOT using --cuda_ray)\")\n",
    "    parser.add_argument('--patch_size', type=int, default=1, help=\"[experimental] render patches in training, so as to apply LPIPS loss. 1 means disabled, use [64, 32, 16] to enable\")\n",
    "    ### network backbone options\n",
    "    parser.add_argument('--fp16', action='store_true', help=\"use amp mixed precision training\")\n",
    "    parser.add_argument('--ff', action='store_true', help=\"use fully-fused MLP\")\n",
    "    parser.add_argument('--tcnn', action='store_true', help=\"use TCNN backend\")\n",
    "    ### dataset options\n",
    "    parser.add_argument('--color_space', type=str, default='srgb', help=\"Color space, supports (linear, srgb)\")\n",
    "    parser.add_argument('--preload', action='store_true', help=\"preload all data into GPU, accelerate training but use more GPU memory\")\n",
    "    # (the default value is for the fox dataset)\n",
    "    parser.add_argument('--bound', type=float, default=2, help=\"assume the scene is bounded in box[-bound, bound]^3, if > 1, will invoke adaptive ray marching.\")\n",
    "    parser.add_argument('--scale', type=float, default=0.33, help=\"scale camera location into box[-bound, bound]^3\")\n",
    "    parser.add_argument('--offset', type=float, nargs='*', default=[0, 0, 0], help=\"offset of camera location\")\n",
    "    parser.add_argument('--dt_gamma', type=float, default=1/128, help=\"dt_gamma (>=0) for adaptive ray marching. set to 0 to disable, >0 to accelerate rendering (but usually with worse quality)\")\n",
    "    parser.add_argument('--min_near', type=float, default=0.2, help=\"minimum near distance for camera\")\n",
    "    parser.add_argument('--density_thresh', type=float, default=10, help=\"threshold for density grid to be occupied\")\n",
    "    parser.add_argument('--bg_radius', type=float, default=-1, help=\"if positive, use a background model at sphere(bg_radius)\")\n",
    "    ### GUI options\n",
    "    parser.add_argument('--gui', action='store_true', help=\"start a GUI\")\n",
    "    parser.add_argument('--W', type=int, default=1920, help=\"GUI width\")\n",
    "    parser.add_argument('--H', type=int, default=1080, help=\"GUI height\")\n",
    "    parser.add_argument('--radius', type=float, default=5, help=\"default GUI camera radius from center\")\n",
    "    parser.add_argument('--fovy', type=float, default=50, help=\"default GUI camera fovy\")\n",
    "    parser.add_argument('--max_spp', type=int, default=64, help=\"GUI rendering max sample per pixel\")\n",
    "    ### experimental\n",
    "    parser.add_argument('--error_map', action='store_true', help=\"use error map to sample rays\")\n",
    "    parser.add_argument('--clip_text', type=str, default='', help=\"text input for CLIP guidance\")\n",
    "    parser.add_argument('--rand_pose', type=int, default=-1, help=\"<0 uses no rand pose, =0 only uses rand pose, >0 sample one rand pose every $ known poses\")\n",
    "\n",
    "\n",
    "\n",
    "    return parser.parse_args(args=[])\n",
    "opt=get_opt()\n",
    "\n",
    "opt.fp16 = True\n",
    "opt.cuda_ray = True\n",
    "opt.preload = True\n",
    "\n",
    "opt.test=True\n",
    "\n",
    "if opt.patch_size > 1:\n",
    "    opt.error_map = False # do not use error_map if use patch-based training\n",
    "    # assert opt.patch_size > 16, \"patch_size should > 16 to run LPIPS loss.\"\n",
    "    assert opt.num_rays % (opt.patch_size ** 2) == 0, \"patch_size ** 2 should be dividable by num_rays.\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the number of parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_name=encoder, parameter=494592\n",
      "module_name=sigma_net, parameter=3072\n",
      "module_name=encoder_dir, parameter=0\n",
      "module_name=color_net, parameter=7168\n",
      "504832\n"
     ]
    }
   ],
   "source": [
    "param=0\n",
    "for name,child in model.named_children():\n",
    "    print(f'module_name={name}, parameter={sum(p.numel() for p in child.parameters() if p.requires_grad)}')\n",
    "    param+= sum(p.numel() for p in child.parameters() if p.requires_grad)\n",
    "\n",
    "print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nerf.provider import *\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Slerp, Rotation\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from nerf.network_tcnn import NeRFNetwork\n",
    "import matplotlib.pyplot as plt\n",
    "from nerf.utils import *\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model=NeRFNetwork(\n",
    "        encoding=\"hashgrid\",\n",
    "        bound=opt.bound,\n",
    "        cuda_ray=opt.cuda_ray,\n",
    "        density_scale=1,\n",
    "        min_near=opt.min_near,\n",
    "        density_thresh=opt.density_thresh,\n",
    "        bg_radius=opt.bg_radius,\n",
    "    ).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_name=encoder, parameter=496240\n",
      "module_name=sigma_net, parameter=3072\n",
      "module_name=encoder_dir, parameter=0\n",
      "module_name=color_net, parameter=7168\n"
     ]
    }
   ],
   "source": [
    "for name,child in model.named_children():\n",
    "    print(f'module_name={name}, parameter={sum(p.numel() for p in child.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nerf.provider import *\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Slerp, Rotation\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from nerf.network_tcnn import NeRFNetwork\n",
    "import matplotlib.pyplot as plt\n",
    "from nerf.utils import *\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "opt.path='./data/photoshapes/shape09096_rank01'\n",
    "opt.cuda_ray=True\n",
    "model = NeRFNetwork(\n",
    "        encoding=\"hashgrid\",\n",
    "        bound=opt.bound,\n",
    "        cuda_ray=opt.cuda_ray,\n",
    "        density_scale=1,\n",
    "        min_near=opt.min_near,\n",
    "        density_thresh=opt.density_thresh,\n",
    "        bg_radius=opt.bg_radius,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeRFNetwork(\n",
       "  (encoder): Encoding(n_input_dims=3, n_output_dims=32, seed=1337, dtype=torch.float16, hyperparams={'base_resolution': 16, 'hash': 'CoherentPrime', 'interpolation': 'Linear', 'log2_hashmap_size': 14, 'n_features_per_level': 2, 'n_levels': 16, 'otype': 'Grid', 'per_level_scale': 1.4472692012786865, 'type': 'Hash'})\n",
       "  (sigma_net): Network(n_input_dims=32, n_output_dims=16, seed=1337, dtype=torch.float16, hyperparams={'encoding': {'offset': 0.0, 'otype': 'Identity', 'scale': 1.0}, 'network': {'activation': 'ReLU', 'n_hidden_layers': 1, 'n_neurons': 64, 'otype': 'FullyFusedMLP', 'output_activation': 'None'}, 'otype': 'NetworkWithInputEncoding'})\n",
       "  (encoder_dir): Encoding(n_input_dims=3, n_output_dims=16, seed=1337, dtype=torch.float16, hyperparams={'degree': 4, 'otype': 'SphericalHarmonics'})\n",
       "  (color_net): Network(n_input_dims=31, n_output_dims=3, seed=1337, dtype=torch.float16, hyperparams={'encoding': {'offset': 0.0, 'otype': 'Identity', 'scale': 1.0}, 'network': {'activation': 'ReLU', 'n_hidden_layers': 2, 'n_neurons': 64, 'otype': 'FullyFusedMLP', 'output_activation': 'None'}, 'otype': 'NetworkWithInputEncoding'})\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "ckpt=torch.load('./data/photoshape_weight/shape09096_rank01/checkpoints/ngp.pth')\n",
    "\n",
    "model.load_state_dict(ckpt['model'],strict=False)\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if you have \"transforms_test.json\" file in path, this code make video of your dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This code is making random pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mwith\u001b[39;00m autocast():\n\u001b[1;32m      9\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 10\u001b[0m         result\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39;49mrender(data[\u001b[39m'\u001b[39;49m\u001b[39mrays_o\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mfloat(),data[\u001b[39m'\u001b[39;49m\u001b[39mrays_d\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mfloat())\n",
      "File \u001b[0;32m~/hyperdiff/nerf/renderer.py:572\u001b[0m, in \u001b[0;36mNeRFRenderer.render\u001b[0;34m(self, rays_o, rays_d, staged, max_ray_batch, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     results[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m image\n\u001b[1;32m    571\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 572\u001b[0m     results \u001b[39m=\u001b[39m _run(rays_o, rays_d, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    574\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/hyperdiff/nerf/renderer.py:363\u001b[0m, in \u001b[0;36mNeRFRenderer.run_cuda\u001b[0;34m(self, rays_o, rays_d, dt_gamma, bg_color, perturb, force_all_rays, max_steps, T_thresh, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m sigmas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdensity_scale \u001b[39m*\u001b[39m sigmas\n\u001b[1;32m    361\u001b[0m raymarching\u001b[39m.\u001b[39mcomposite_rays(n_alive, n_step, rays_alive, rays_t, sigmas, rgbs, deltas, weights_sum, depth, image, T_thresh)\n\u001b[0;32m--> 363\u001b[0m rays_alive \u001b[39m=\u001b[39m rays_alive[rays_alive \u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m]\n\u001b[1;32m    365\u001b[0m \u001b[39m#print(f'step = {step}, n_step = {n_step}, n_alive = {n_alive}, xyzs: {xyzs.shape}')\u001b[39;00m\n\u001b[1;32m    367\u001b[0m step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m n_step\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "poses=rand_poses(5,device,theta_range=[np.pi/3,np.pi/3],radius=0.5)\n",
    "# make intrinsic\n",
    "focal=245/(2*np.tan(np.radians(60)/2))\n",
    "intrinsics=np.array([focal,focal,256//2,256//2])\n",
    "data=get_rays(poses,intrinsics,256,256)\n",
    "\n",
    "with autocast():\n",
    "    with torch.no_grad():\n",
    "        result=model.render(data['rays_o'].float(),data['rays_d'].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=result['image'].reshape(-1,256,256,3)\n",
    "pred=srgb_to_linear(pred).detach().cpu().numpy()\n",
    "pred=(pred*255).astype(np.uint8)\n",
    "\n",
    "cv2.imwrite(os.path.join(\"/home/poong/hyperdiff\",'sample1.png'), cv2.cvtColor(pred[3], cv2.COLOR_RGB2BGR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data\u001b[39m=\u001b[39mget_rays(poses,intrinsics,\u001b[39m256\u001b[39;49m,\u001b[39m256\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m \u001b[39mwith\u001b[39;00m autocast():\n\u001b[1;32m      3\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/anaconda/poong/anaconda3/envs/hyperdiff/lib/python3.9/site-packages/torch/amp/autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_autocast\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     13\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 14\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/hyperdiff/nerf/utils.py:129\u001b[0m, in \u001b[0;36mget_rays\u001b[0;34m(poses, intrinsics, H, W, N, error_map, patch_size)\u001b[0m\n\u001b[1;32m    127\u001b[0m directions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack((xs, ys, zs), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    128\u001b[0m directions \u001b[39m=\u001b[39m directions \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39mnorm(directions, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 129\u001b[0m rays_d \u001b[39m=\u001b[39m directions \u001b[39m@\u001b[39;49m poses[:, :\u001b[39m3\u001b[39;49m, :\u001b[39m3\u001b[39;49m]\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m) \u001b[39m# (B, N, 3)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m rays_o \u001b[39m=\u001b[39m poses[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m] \u001b[39m# [B, 3]\u001b[39;00m\n\u001b[1;32m    132\u001b[0m rays_o \u001b[39m=\u001b[39m rays_o[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39mNone\u001b[39;00m, :]\u001b[39m.\u001b[39mexpand_as(rays_d) \u001b[39m# [B, N, 3]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched( handle, opa, opb, m, n, k, &alpha, a, lda, stridea, b, ldb, strideb, &beta, c, ldc, stridec, num_batches)`"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data=get_rays(poses,intrinsics,256,256)\n",
    "with autocast():\n",
    "    with torch.no_grad():\n",
    "        result=model.render(data['rays_o'].float(),data['rays_d'].float())\n",
    "\n",
    "pred=result['image'].reshape(-1,256,256,3)\n",
    "pred=srgb_to_linear(pred).detach().cpu().numpy()\n",
    "pred=(pred*255).astype(np.uint8)\n",
    "\n",
    "cv2.imwrite(os.path.join(\"/home/poong/hyperdiff\",'sample.png'), cv2.cvtColor(pred[0], cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'intrinsics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m poses\u001b[39m=\u001b[39mline_poses(\u001b[39m10\u001b[39m,device,radius\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m data\u001b[39m=\u001b[39mget_rays(poses,intrinsics,\u001b[39m256\u001b[39m,\u001b[39m256\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m autocast():\n\u001b[1;32m      4\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'intrinsics' is not defined"
     ]
    }
   ],
   "source": [
    "poses=line_poses(10,device,radius=0.5)\n",
    "data=get_rays(poses,intrinsics,256,256)\n",
    "with autocast():\n",
    "    with torch.no_grad():\n",
    "        result=model.render(data['rays_o'].float(),data['rays_d'].float())\n",
    "\n",
    "pred=result['image'].reshape(-1,256,256,3)\n",
    "pred=srgb_to_linear(pred).detach().cpu().numpy()\n",
    "pred=(pred*255).astype(np.uint8)\n",
    "\n",
    "#cv2.imwrite(os.path.join(\"/home/poong/hyperdiff\",'sample0.png'), cv2.cvtColor(pred[3], cv2.COLOR_RGB2BGR))\n",
    "imageio.mimwrite(os.path.join(\"/home/poong/hyperdiff\",'sample.mp4'),pred, fps=2, quality=8, macro_block_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [PSNRMeter(), LPIPSMeter(device=device)]\n",
    "test_loader = NeRFDataset(opt, device=device, type='test').dataloader()\n",
    "trainer = Trainer('ngp', opt, model, device=device, workspace=opt.workspace, fp16=opt.fp16, metrics=metrics, use_checkpoint=opt.ckpt)\n",
    "trainer.test(test_loader, write_video=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerf.provider import *\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Slerp, Rotation\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from nerf.network_tcnn import NeRFNetwork\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "opt=get_opt()\n",
    "device = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seed_everything' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m seed_everything(opt\u001b[39m.\u001b[39mseed)\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m NeRFNetwork(\n\u001b[1;32m      3\u001b[0m         encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhashgrid\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m         bound\u001b[39m=\u001b[39mopt\u001b[39m.\u001b[39mbound,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m         bg_radius\u001b[39m=\u001b[39mopt\u001b[39m.\u001b[39mbg_radius,\n\u001b[1;32m     10\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seed_everything' is not defined"
     ]
    }
   ],
   "source": [
    "seed_everything(opt.seed)\n",
    "model = NeRFNetwork(\n",
    "        encoding=\"hashgrid\",\n",
    "        bound=opt.bound,\n",
    "        cuda_ray=opt.cuda_ray,\n",
    "        density_scale=1,\n",
    "        min_near=opt.min_near,\n",
    "        density_thresh=opt.density_thresh,\n",
    "        bg_radius=opt.bg_radius,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-7.0460e-05, -3.4031e-05,  3.8013e-05,  ..., -1.4546e-05,\n",
       "         2.6831e-05, -4.1242e-05], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tinycudann as tcnn\n",
    "encoder = tcnn.Encoding(\n",
    "    n_input_dims=3,\n",
    "    seed=1337,\n",
    "    encoding_config={\n",
    "        \"otype\": \"HashGrid\",\n",
    "        \"n_levels\": 16,\n",
    "        \"n_features_per_level\": 2,\n",
    "        \"log2_hashmap_size\": 14,\n",
    "        \"base_resolution\": 16,\n",
    "        \"per_level_scale\": np.exp2(np.log2(2048 * 1/ 16) / (16 - 1)),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt=torch.load('./result/shape09096_rank00/checkpoints/ngp.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7593666911125183"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['mean_density']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nerf.provider import *\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Slerp, Rotation\n",
    "import math\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "from nerf.network_tcnn import NeRFNetwork\n",
    "import matplotlib.pyplot as plt\n",
    "from nerf.utils import *\n",
    "from weight_encoder import *\n",
    "ckpt=torch.load('./data/photoshape_weight/shape09096_rank00/checkpoints/ngp.pth')\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['aabb_train', 'aabb_infer', 'density_bitfield', 'step_counter', 'encoder.params', 'sigma_net.params', 'encoder_dir.params', 'color_net.params'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['model'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param=torch.concat((ckpt['model']['encoder.params'],ckpt['model']['sigma_net.params'],ckpt['model']['color_net.params']),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer=TransformerEncoderLayer(d_model=512,nhead=8).to(device)\n",
    "#transformer_encoder=torch.nn.TransformerEncoder(encoder_layer,num_layers=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986\n",
      "986\n",
      "[512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512]\n"
     ]
    }
   ],
   "source": [
    "encoded=Transformer_Encoder(param.size(),param.size(), chunk_size=512)(param.unsqueeze(0)).cpu()                                   \n",
    "#positioned=PositionalEncoding(512)(encoded)\n",
    "#out=transformer_encoder(positioned.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer=torch.nn.TransformerDecoderLayer(d_model=512,nhead=8)\n",
    "decoder=torch.nn.TransformerDecoder(decoder_layer,num_layers=6).to(device)\n",
    "src=torch.rand_like(out)\n",
    "result=decoder(src,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 986, 256])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampling=nn.Linear(512,256).to(device)\n",
    "downsampling(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([494592])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt['model']['encoder.params'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5999,  1.3323,  0.3148,  ..., -1.6161,  1.4250, -1.2095],\n",
       "       device='cuda:5', grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "index1=len(ckpt['model']['encoder.params'])\n",
    "index2=index1+len(ckpt['model']['sigma_net.params'])\n",
    "index3=index2+len(ckpt['model']['color_net.params'])\n",
    "ckpt['model']['encoder.params']=result.flatten()[:index1]\n",
    "ckpt['model']['sigma_net.params']=result.flatten()[index1:index2]\n",
    "ckpt['model']['color_net.params']=result.flatten()[index2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ckpt,'hi.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 986, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986\n",
      "986\n"
     ]
    }
   ],
   "source": [
    "from weight_encoding.transformer.encoder import *\n",
    "from weight_encoding.transformer.block import Weight_Split,DecoderLayer\n",
    "from nerf.provider import *\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "from nerf.network_tcnn import NeRFNetwork\n",
    "import matplotlib.pyplot as plt\n",
    "from nerf.utils import *\n",
    "\n",
    "\n",
    "ckpt=torch.load('./data/photoshape_weight/shape09096_rank00/checkpoints/ngp.pth')\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "param=torch.concat((ckpt['model']['encoder.params'],ckpt['model']['sigma_net.params'],ckpt['model']['color_net.params']),-1)\n",
    "splited_param=Weight_Split(param.size(),param.size(), chunk_size=512)(param.unsqueeze(0)).to(device)                               \n",
    "\n",
    "encoder = Encoder(986,512,2048,8,8,0.1,device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded=encoder(splited_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderLayer(32,64,8,0.1,out=64).to(device)\n",
    "decoded = decoder(encoded,encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyperdiff1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
